# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRCknsuxiVIbkhvwdpCEwdxPBb_5CySr
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold
from sklearn.preprocessing import LabelEncoder


# -------------------------
# Load Dataset
# -------------------------
df = pd.read_csv("/content/Student_Performance.csv")

# -------------------------
# Load & Preprocess Data
# -------------------------
target_col = "Performance Index"   # <-- change if dataset differs

# Encode target
if df[target_col].dtype == "object":
    le = LabelEncoder()
    y = le.fit_transform(df[target_col])
else:
    y = df[target_col].values

# Features
X = df.drop(columns=[target_col])
X = pd.get_dummies(X, drop_first=True).values

# -------------------------
# Safe CV Splitter
# -------------------------
def get_cv(y, task="classification", n_splits=3):
    if task == "classification":
        min_class_count = np.min(np.bincount(y))
        # Ensure splits â‰¤ smallest class count
        n_splits = min(n_splits, min_class_count) if min_class_count > 1 else 2
        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    else:
        return KFold(n_splits=n_splits, shuffle=True, random_state=42)

# -------------------------
# Objective Function
# -------------------------
def evaluate(params):
    C = float(params[0])

    # Detect task type
    task = "classification" if len(np.unique(y)) < 20 else "regression"

    if task == "classification":
        model = LogisticRegression(solver="liblinear", C=C, max_iter=300)
        cv = get_cv(y, task="classification", n_splits=3)
        scores = cross_val_score(model, X, y, cv=cv, scoring="accuracy")
    else:
        model = LinearRegression()
        cv = get_cv(y, task="regression", n_splits=3)
        scores = cross_val_score(model, X, y, cv=cv, scoring="r2")

    return -scores.mean()   # minimize

# -------------------------
# Gradient Descent (GD)
# -------------------------
def gradient_descent(obj, x0, lr=0.1, steps=30):
    x = np.array(x0, dtype=float)
    for _ in range(steps):
        grad = np.zeros_like(x)
        fx = obj(x)
        eps = 1e-4
        for i in range(len(x)):
            xx = x.copy()
            xx[i] += eps
            grad[i] = (obj(xx) - fx) / eps
        x -= lr * grad
        x = np.clip(x, [0.01], [10])  # keep C in range
    return x, -obj(x)

# -------------------------
# Simulated Annealing (SA)
# -------------------------
def simulated_annealing(obj, x0, steps=100, T0=1.0, alpha=0.95):
    x = np.array(x0, dtype=float)
    fx = obj(x)
    best, best_f = x.copy(), fx
    T = T0
    for _ in range(steps):
        xn = x + np.random.normal(0, 0.5, size=len(x))
        xn = np.clip(xn, [0.01], [10])
        fn = obj(xn)
        if fn < fx or np.random.rand() < np.exp(-(fn-fx)/T):
            x, fx = xn, fn
        if fn < best_f:
            best, best_f = xn, fn
        T *= alpha
    return best, -best_f

# -------------------------
# Genetic Algorithm (GA)
# -------------------------
def genetic_algorithm(obj, pop_size=10, gens=10):
    pop = np.random.uniform([0.01], [10], size=(pop_size, 1))
    fitness = np.array([obj(ind) for ind in pop])

    for _ in range(gens):
        parents = []
        for _ in range(pop_size):
            i, j = np.random.randint(0, pop_size, 2)
            parents.append(pop[i] if fitness[i] < fitness[j] else pop[j])
        parents = np.array(parents)

        children = parents + np.random.normal(0, 0.2, size=parents.shape)
        children = np.clip(children, [0.01], [10])

        child_fit = np.array([obj(c) for c in children])
        combined = np.vstack([pop, children])
        comb_fit = np.hstack([fitness, child_fit])

        idx = np.argsort(comb_fit)[:pop_size]
        pop, fitness = combined[idx], comb_fit[idx]

    return pop[0], -fitness[0]



# -------------------------
# Run All Algorithms
# -------------------------
gd_best, gd_acc = gradient_descent(evaluate, [1.0])
sa_best, sa_acc = simulated_annealing(evaluate, [5.0])
ga_best, ga_acc = genetic_algorithm(evaluate)
pso_best, pso_acc = pso(evaluate)

print("\nGradient Descent:")
print("C =", gd_best)
print("Accuracy Score =", f"{gd_acc*100:.2f}%")

print("\nSimulated Annealing:")
print("C =", sa_best)
print("Accuracy Score =", f"{sa_acc*100:.2f}%")

print("\nGenetic Algorithm:")
print("C =", ga_best)
print("Accuracy Score =", f"{ga_acc*100:.2f}%")

# -------------------------
# Find Best Algorithm
# -------------------------
results = {
    "GD": (gd_best, gd_acc),
    "SA": (sa_best, sa_acc),
    "GA": (ga_best, ga_acc),
    
}

best_algo = max(results.items(), key=lambda x: x[1][1])
print("\nBest Algorithm =", best_algo[0])
print("Best C =", best_algo[1][0])
print("Best Score =", best_algo[1][1])