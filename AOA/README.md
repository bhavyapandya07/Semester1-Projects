**🚀 Optimization Algorithms Project**



This project explores classical optimization (Gradient Descent) and metaheuristic optimization algorithms (Simulated Annealing, Genetic Algorithm, Particle Swarm Optimization) applied to two domains:

**Machine Learning (Logistic Regression)** – to optimize the regularization parameter (C) using different algorithms and compare their performance.

**Traveling Salesman Problem (TSP)** – a classic NP-hard problem, solved using SA and GA to analyze speed, accuracy, and solution quality.

The project highlights how different algorithms perform on convex problems (ML models) versus non-convex problems (TSP).





**⚡ Features**



✅ Implementation of Gradient Descent, SA, GA, and PSO for ML optimization

✅ Comparison of time complexity, accuracy, and convergence across algorithms

✅ Case study using Student Performance Dataset (Logistic Regression)

✅ Full implementation of TSP using SA, GA, and PSO with convergence plots

✅ Visualization of optimization paths and algorithm performance





**🛠️ Algorithms Implemented**



**Gradient Descent (GD)** → Fast \& reliable for convex functions

**Simulated Annealing (SA)** → Escapes local minima with probabilistic jumps

**Genetic Algorithm (GA)** → Evolves solutions using crossover \& mutation





**📊 Key Insights**



GD performs best for convex ML tasks (fast \& optimal).

SA is fastest for TSP but less accurate.

GA gives the most accurate TSP solutions but takes more time.



Dataset Link:

https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression





**📑 Notes**



Dataset used: Student Performance Dataset

Libraries required: numpy, pandas, scikit-learn, matplotlib

Run scripts separately for ML optimization and TSP optimization

Visual outputs include paths for TSP and convergence plots for all algorithms

