**ğŸš€ Optimization Algorithms Project**



This project explores classical optimization (Gradient Descent) and metaheuristic optimization algorithms (Simulated Annealing, Genetic Algorithm, Particle Swarm Optimization) applied to two domains:

**Machine Learning (Logistic Regression)** â€“ to optimize the regularization parameter (C) using different algorithms and compare their performance.

**Traveling Salesman Problem (TSP)** â€“ a classic NP-hard problem, solved using SA and GA to analyze speed, accuracy, and solution quality.

The project highlights how different algorithms perform on convex problems (ML models) versus non-convex problems (TSP).





**âš¡ Features**



âœ… Implementation of Gradient Descent, SA, GA, and PSO for ML optimization

âœ… Comparison of time complexity, accuracy, and convergence across algorithms

âœ… Case study using Student Performance Dataset (Logistic Regression)

âœ… Full implementation of TSP using SA, GA, and PSO with convergence plots

âœ… Visualization of optimization paths and algorithm performance





**ğŸ› ï¸ Algorithms Implemented**



**Gradient Descent (GD)** â†’ Fast \& reliable for convex functions

**Simulated Annealing (SA)** â†’ Escapes local minima with probabilistic jumps

**Genetic Algorithm (GA)** â†’ Evolves solutions using crossover \& mutation





**ğŸ“Š Key Insights**



GD performs best for convex ML tasks (fast \& optimal).

SA is fastest for TSP but less accurate.

GA gives the most accurate TSP solutions but takes more time.



Dataset Link:

https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression





**ğŸ“‘ Notes**



Dataset used: Student Performance Dataset

Libraries required: numpy, pandas, scikit-learn, matplotlib

Run scripts separately for ML optimization and TSP optimization

Visual outputs include paths for TSP and convergence plots for all algorithms

